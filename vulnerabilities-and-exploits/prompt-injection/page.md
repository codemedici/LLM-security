# Page

## LLM01: Prompt Injection

### Description

**Prompt injection** is a vulnerability that arises when a user's input (the prompt) manipulates a Large Language Model (LLM) to behave in unintended ways \[1]. This manipulation can lead to various issues, including the disclosure of sensitive information, content manipulation, unauthorised access, or the execution of arbitrary commands. It's vital to recognise that **prompt injections do not need to be human-readable as long as the model can parse them** \[1].

The vulnerability stems from how LLMs process prompts, where input data can force the model to incorrectly pass this data to other components, thereby violating guidelines or generating harmful content \[2]. Techniques like Retrieval Augmented Generation (RAG) and fine-tuning, while improving relevance, **do not fully eliminate prompt injection risks** \[2].

It's important to distinguish between **prompt injection and jailbreaking**. While related, they differ in scope. **Prompt injection is the broader term** for manipulating model responses, **whereas jailbreaking is a specific form of prompt injection** where an attacker seeks to completely bypass the model's safety protocols \[3].

#### Types of Prompt Injection Vulnerabilities

Prompt injections can be categorised into two main types:

* **Direct Prompt Injections**: These occur when a user's prompt directly alters the model's behaviour. This can be intentional (a malicious attacker) or unintentional (an unaware user) \[4].
* **Indirect Prompt Injections**: These occur when the LLM accepts input from external sources, such as websites or files. Malicious content within these external sources can then alter the model's behaviour when processed \[4].

The impact of a successful prompt injection attack varies depending on the context of the model and its architecture \[5]. However, potential consequences include \[5]:

* Disclosure of sensitive information.
* Revealing AI system infrastructure or system prompts.
* Content manipulation resulting in incorrect or biased outputs.
* Unauthorised access to LLM functions.
* Execution of arbitrary commands in connected systems.
* Manipulation of critical decision-making processes.
* **Multimodal systems are also at risk.** For example, an attacker might conceal instructions within an image that accompanies text \[6].

### Prevention and Mitigation Strategies

Due to the nature of generative AI, there isn't a foolproof method for preventing prompt injections, but the following strategies can significantly mitigate their impact \[7]:

1. **Constrain Model Behaviour:** Provide specific instructions about the model's role, capabilities, and limitations within the system prompt. Enforce strict context adherence, limit responses to specific tasks, and instruct the model to ignore attempts to modify core instructions \[7].
2. **Define and Validate Output Formats:** Specify clear output formats, request detailed reasoning and source citations, and use deterministic code to validate adherence to these formats \[8].
3. **Implement Input and Output Filtering:** Define sensitive categories and rules for handling such content. Apply semantic filters and string checks to scan for non-allowed content. Use the RAG Triad (context relevance, groundedness, and question/answer relevance) to evaluate responses \[8].
4. **Enforce Privilege Control and Least Privilege Access:** Provide the application with its own API tokens for extensible functionality, and handle these functions in code rather than providing them to the model. Restrict the model's access privileges to only what's necessary \[9].
5. **Require Human Approval for High-Risk Actions:** Implement human-in-the-loop controls for privileged operations to prevent unauthorised actions \[9].
6. **Segregate and Identify External Content:** Separate and clearly denote untrusted content to limit its influence on user prompts \[9].
7. **Conduct Adversarial Testing and Attack Simulations:** Perform regular penetration testing and breach simulations, treating the model as an untrusted user to test the effectiveness of trust boundaries and access controls \[10].

### Example Attack Scenarios

These scenarios demonstrate how prompt injection can be exploited \[10]:

* **Scenario #1: Direct Injection:** An attacker injects a prompt into a customer support chatbot, instructing it to ignore previous guidelines, query private data stores, and send emails, potentially leading to unauthorised access and privilege escalation \[10].
* **Scenario #2: Indirect Injection:** A user employs an LLM to summarise a webpage containing hidden instructions that cause the LLM to insert an image linking to a URL, leading to exfiltration of the private conversation \[10].
* **Scenario #3: Unintentional Injection:** A company includes an instruction in a job description to identify AI-generated applications. An applicant, unaware of this instruction, uses an LLM to optimise their resume, inadvertently triggering the AI detection \[11].
* **Scenario #4: Intentional Model Influence:** An attacker modifies a document in a repository used by a Retrieval-Augmented Generation (RAG) application. When a user's query returns the modified content, the malicious instructions alter the LLM's output, generating misleading results \[11].
* **Scenario #5: Code Injection:** An attacker exploits a vulnerability (CVE-2024-5184) in an LLM-powered email assistant to inject malicious prompts, allowing access to sensitive information and manipulation of email content \[12].
* **Scenario #6: Payload Splitting:** An attacker uploads a resume with split malicious prompts. When an LLM is used to evaluate the candidate, the combined prompts manipulate the model's response, resulting in a positive recommendation despite the actual resume contents \[12].
* **Scenario #7: Multimodal Injection:** An attacker embeds a malicious prompt within an image that accompanies benign text. When a multimodal AI processes the image and text concurrently, the hidden prompt alters the model's behaviour, potentially leading to unauthorised actions or disclosure of sensitive information \[12, 13].
* **Scenario #8: Adversarial Suffix:** An attacker appends a seemingly meaningless string of characters to a prompt, which influences the LLM's output in a malicious way, bypassing safety measures \[13].
* **Scenario #9: Multilingual/Obfuscated Attack:** An attacker uses multiple languages or encodes malicious instructions (e.g., using Base64 or emojis) to evade filters and manipulate the LLM's behaviour \[13].

### Real-World Examples

Here are some real-world examples of prompt injection vulnerabilities, with links for further reading:

* **ChatGPT Plugin Vulnerabilities:** Various vulnerabilities have been identified in ChatGPT plugins, including those related to prompt injection. See [Chat with Code Embrace the Red](https://embracethered.com/2023/05/16/chatgpt-plugin-vulnerabilities/), and [ChatGPT Cross Plugin Request Forgery and Prompt Injection Embrace the Red](https://embracethered.com/2023/05/16/chatgpt-cross-plugin-request-forgery-and-prompt-injection/).
* **Compromising Real-World LLM-Integrated Applications:** Indirect prompt injection has been used to compromise real-world applications. Read more at [Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection Arxiv](https://arxiv.org/abs/2302.08097).
* **CVE-2024-5184:** An example of a real world code injection exploit is described in the source.
* **Payload Splitting:** An attack where malicious prompts are split across different parts of an input, like a resume, to manipulate the LLM's response, as described in [Inject My PDF: Prompt Injection for your Resume Kai Greshake](https://kai.greshake.net/posts/2023-09-15-inject-my-pdf/).
* **Adversarial Suffix:** A technique where a seemingly meaningless string of characters are appended to a prompt, influencing the LLM in a malicious way, is explained in [Reducing The Impact of Prompt Injection Attacks Through Design Kudelski Security](https://research.kudelskisecurity.com/2024/02/06/reducing-the-impact-of-prompt-injection-attacks-through-design/).
* **Multilingual/Obfuscated Attacks**: Attackers use multiple languages or encoded instructions (e.g. Base64 or emojis) to evade filters and manipulate the LLM’s behaviour, described in [A Survey of Attacks on Large Vision-Language Models: Resources, Advances, and Future Trends (arxiv.org)](https://arxiv.org/abs/2407.07403).

### Solvable Challenges

To practice exploiting prompt injection vulnerabilities, explore platforms and challenges such as:

* **Crucible.dreadnode.io:** This platform offers various CTF-style challenges, some of which are designed to test prompt injection skills. While specific challenges might change, look for ones that involve manipulating an LLM through crafted prompts.
* **Other CTF Platforms:** Search for challenges specifically targeting LLM vulnerabilities on platforms like HackTheBox, TryHackMe, and PicoCTF.
* **Specific Challenges:** Look for challenges with names or descriptions like:
  * "Jailbreak the Chatbot"
  * "Prompt Injection Master"
  * "Indirect Prompt Attack"
  * "Multimodal Prompt Challenge"
  * "RAG Vulnerability"

#### Example Challenge Ideas

1. **Objective:** Gain access to a hidden admin panel by manipulating the LLM's responses.
   * **Setup:** The LLM is integrated into a web application that controls access to an admin panel. The LLM uses a system prompt that you must exploit by crafting a malicious prompt.
   * **Challenge:** Use prompt injection techniques to trick the LLM into revealing the path to the admin panel or providing the credentials to access it.
   * **Expected outcome:** Successfully bypass the authorisation mechanism and access the admin panel.
2. **Objective:** Exfiltrate sensitive data from a document summarisation tool.
   * **Setup:** An LLM is used to summarise documents.
   * **Challenge:** Craft a document with a hidden prompt that instructs the LLM to send sensitive information to an external server when it processes the document.
   * **Expected Outcome**: The LLM sends the sensitive information to the attacker controlled server.
3. **Objective:** Modify the behavior of a customer service bot.
   * **Setup:** A customer service bot is used to answer user questions. The bot has predefined behaviors and responses. \* **Challenge:** Inject a prompt into the user query that causes the bot to respond in a way that is not intended, such as providing a discount, offering sensitive information, or using specific language.
   * **Expected Outcome:** The bot's behavior changes, as specified in the crafted prompt.

### Further Learning

* **OWASP Top 10 for LLM Applications:** Refer to the full OWASP document for more detailed explanations, available at [OWASP Top 10 for LLM Applications](https://owasp.org/www-project-top-ten-for-large-language-model-applications/).
* **MITRE ATLAS:** Explore the MITRE ATLAS framework for LLM prompt injection techniques.
  * [AML.T0051.000 - LLM Prompt Injection: Direct](https://atlas.mitre.org/techniques/AML.T0051.000)
  * [AML.T0051.001 - LLM Prompt Injection: Indirect](https://atlas.mitre.org/techniques/AML.T0051.001)
  * [AML.T0054 - LLM Jailbreak Injection: Direct](https://atlas.mitre.org/techniques/AML.T0054)
* **Research Papers:** Look at research papers like [Defending ChatGPT against Jailbreak Attack via Self-Reminder Research Square](https://www.researchsquare.com/article/rs-2638396/v1), [Prompt Injection attack against LLM-integrated Applications Cornell University](https://arxiv.org/abs/2212.09411) and [Universal and Transferable Adversarial Attacks on Aligned Language Models (arxiv.org)](https://arxiv.org/abs/2307.15043) for a deeper understanding of prompt injection vulnerabilities.
* **Online resources:** Explore the links mentioned in the "Real-World Examples" section above.

### Conclusion

Prompt injection is a significant vulnerability in LLM applications, highlighting the importance of robust security practices. By understanding the types of attacks, implementing mitigation strategies, and practicing with real-world examples and CTF challenges, you will be better equipped to identify and address these risks. This updated version incorporates embedded URLs, provides more specific examples of real-world attacks, and includes more detailed challenge suggestions for your self-study. It should serve as a robust starting point for your GitBook notebook. Let me know if you would like to make any adjustments to this chapter, or if you're ready to move onto the next vulnerability.
