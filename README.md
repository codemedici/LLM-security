# README

## LLM Security Notebook: AI Red Teaming Edition

Welcome to the **LLM Security Notebook**, a comprehensive resource tailored for:

* ðŸ›¡ï¸ **AI Red Teamers**
* ðŸ‘©â€ðŸ’» **Penetration Testers targeting LLMs**
* ðŸ§  **Security Researchers**
* â˜ï¸ **Engineers deploying LLMs in production**

This GitBook/Markdown repository is the result of months of research, book parsing, tool experimentation, and practical organization of LLM security knowledge into a living knowledge base.

***

### ðŸ“ Repository Structure

```
llm-security-notebook/
â”‚
â”œâ”€â”€ 01-fundamentals/               # Core concepts: taxonomy, attack types, threat modeling
â”œâ”€â”€ 02-threats-attacks/           # Injection, inference, evasion, extraction, etc.
â”œâ”€â”€ 03-evaluation-hardening/      # Red teaming, testing, adversarial defenses
â”œâ”€â”€ 04-supply-chain-risks/        # Model poisoning, backdoors, serialized payloads
â”œâ”€â”€ 05-platform-surfaces/         # Colab, Sagemaker, Azure, Edge AI, multi-tenant infra
â”œâ”€â”€ 06-detection-monitoring/      # Observability, logging, behavioral analysis
â”œâ”€â”€ 07-tools-techniques/          # PyRIT, Garak, LLMGuard, ART, etc.
â”œâ”€â”€ 08-case-studies/              # Real-world attacks, incident postmortems
â”œâ”€â”€ 09-defensive-strategies/      # Access control, fine-tuning safety, sandboxing
â”œâ”€â”€ 10-reference-guides/          # Ports, services, scanners, wordlists, cheat sheets
â””â”€â”€ README.md                     # This file
```

Each folder maps to a section in GitBook and mirrors a topic cluster for practitioners.

***

### ðŸ”„ Syncing Between GitBook and GitHub

If using this repo as the **source of truth**:

* Use the **GitBook GitHub sync integration**
* Set the `main` branch as source

Alternatively, this repo can remain as an archive of `.md` files while content is edited in GitBook.

***

### ðŸ¤ Contributing / Collaboration

Suggestions, new attack techniques, tool usage examples, and code snippets are welcome! Please open an issue or PR with details.

Future collaboration may involve:

* Parsing more LLM security books, academic papers, or slides
* Reverse engineering LLM attack demos from CTFs or bug bounty writeups
* Extracting structured notes from video lectures or conference talks

***

### ðŸ§  Origin and Purpose

This notebook was seeded from:

* 6+ major LLM security books
* OWASP GenAI Guide
* MITRE ATLAS
* NIST AI 100
* Real-world red teaming & pentest scenarios
* Extensive brainstorming and design

Its purpose is to be **your daily companion** in understanding, attacking, and defending large language models.

***

### ðŸ“¬ Contact

Maintainer: [Cosimo de' Medici](https://www.linkedin.com/in/codemedici/)\
For questions, reach out or open an issue.

***

> _"Never trust, always verify"â€”especially when your model is talking back._
