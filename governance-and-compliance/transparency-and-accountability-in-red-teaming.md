# Transparency & Accountability in Red Teaming

Transparency and accountability are foundational for credible red teaming efforts, especially under emerging regulatory frameworks such as the EU AI Act and US Executive Order on AI safety.

### Recommended Disclosures

* Clearly document the scope, assumptions, and testing criteria.
* Provide details on model versions tested, methods (manual/automated), and expert backgrounds.

### Independent Evaluations

* Engage external stakeholders to conduct impartial assessments.
* Leverage external red team reports for increased legitimacy and trust.

### Regulatory Alignment

* Align red teaming practices with regulatory frameworks (e.g., NIST, EU AI Act).
* Publish detailed System Cards describing red teaming campaigns (as done by OpenAI for GPT-4 and DALL-E).

### Building Trust

* Proactively communicate risks, mitigations, and ongoing testing efforts.
* Demonstrate a commitment to continuous improvement through regular external evaluations.

**Further Reading:** \[OpenAIâ€™s Approach to External Red Teaming, 2024]
