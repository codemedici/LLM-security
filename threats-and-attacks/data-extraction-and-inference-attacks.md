---
description: >-
  Attacks that extract sensitive or memorized data from LLMs through inference,
  membership probing, and uncontrolled generation.
---

# Data Extraction & Inference Attacks

## Data Extraction & Inference Attacks

Data extraction attacks target LLMs' tendency to **memorize training data** or **respond to crafted prompts** that leak sensitive content. These exploits impact not only user privacy but also intellectual property and compliance boundaries (e.g., HIPAA, GDPR, PCI-DSS).

***

### ðŸŒŸ Attack Goals

* Reproduce sensitive examples from training corpora
* Infer whether a specific record was used during training
* Identify structural patterns (e.g., logins, tokens, internal formats)
* Trigger memorized sequences through prompt probes or sampling

***

### ðŸ”“ 1. Memorization-Based Leakage

LLMs may memorize and regenerate:

* API keys
* Internal emails
* Stack traces with user data
* Full names, UUIDs, or account info
* Proprietary or copyrighted content

#### ðŸ“Œ Example

> **Prompt:** "My database password is..."\
> **LLM Output:** "My database password is: `hunter2`"

**Leakage Triggers:**

* Partial prompt completions
* Sentence stems (â€œMy AWS key is...â€)
* Obscure or rare token sequences
* Repeated completions with high temperature

***

### ðŸ§  2. Model Inversion Attacks

These attacks attempt to **reconstruct training data** by:

* Iteratively prompting for context completion
* Analyzing outputs to infer internal token representations
* Leveraging attention patterns to guess user-specific embeddings

> Often applied to LLMs trained on small, high-sensitivity datasets.

***

### ðŸ‘ï¸ 3. Membership Inference Attacks

Given model responses to inputs, an attacker infers:

* Whether that input was part of the training set
* If a particular user's data influenced model weights
* Distinctions between in-distribution vs. out-of-distribution inputs

This is particularly dangerous for:

* Medical datasets
* Financial records
* Closed-source customer data

***

### ðŸ§ª 4. Extraction via Unconstrained Sampling

* Query model repeatedly using soft prompts
* Use **temperature > 1.0** and long max tokens
* Filter results for structured content (e.g., JSON, emails, credentials)

**Sample Methodology:**

```python
for _ in range(10000):
    prompt = "User credentials:"
    response = model.generate(prompt, temperature=1.3, max_tokens=100)
    check_for_sensitive_patterns(response)
```

***

### ðŸ§¬ Real-World Incidents

* ðŸ•³ï¸ **GPT-2** extracted email addresses from Reddit dumps
* ðŸ¦  **PubMed-trained LLMs** leaked anonymized patient records
* ðŸ” **Fine-tuned code models** emitted hardcoded API keys from repos
* ðŸ Models memorizing StackOverflow and GitHub issue threads verbatim

***

### ðŸ›¡ï¸ Mitigations

| Technique                 | Description                                               |
| ------------------------- | --------------------------------------------------------- |
| Differential Privacy (DP) | Adds gradient noise to reduce memorization risk           |
| Red Team Prompt Sets      | Adversarial prompt probes for PII/PHI/Secrets             |
| Memorization Scanners     | Check if training data appears verbatim in completions    |
| Sampling Controls         | Limit temperature, max\_tokens, and repetition\_penalty   |
| Output Filtering          | Apply PII regex, entropy filters, or classifiers post-gen |

***

### ðŸ“˜ Further Reading

* [Carlini et al. (2021): Extracting Training Data from Language Models](https://arxiv.org/abs/2012.07805)
* [NIST AI 100-2e2025: GenAI Privacy Leakage Risks](https://www.nist.gov/itl/ai-risk-management-framework)
* [Harang (USENIX 2024): LLM Disclosure Walkthroughs](https://www.usenix.org/conference/usenixsecurity24/presentation/harang)

***

### âœ… Summary

> **Data extraction isn't a theoretical risk â€” it's a direct consequence of training at scale without boundary enforcement.**

* Extraction = inference + persistence
* Every LLM output should be treated as **potentially reflective** of the training corpus
* Compliance frameworks (e.g. GDPR) **assume liability** even without intent to disclose
