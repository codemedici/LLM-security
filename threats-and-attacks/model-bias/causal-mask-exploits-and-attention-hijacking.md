---
description: >-
  Attention hijacking and causal masking exploits in decoder-only transformer
  architectures like GPT, with attacker techniques and detection strategies.
---

# Causal Mask Exploits and Attention Hijacking

## Causal Mask Exploits and Attention Hijacking

In decoder-only transformer models like GPT, a **causal mask** ensures each token only attends to its past context — never to the future. This is critical for maintaining autoregressive behavior and enforcing alignment boundaries.

***

### 🧠 What Is a Causal Mask?

Causal masks are implemented by zeroing out upper-triangle values in the attention matrix:

```python
mask = torch.tril(torch.ones(seq_len, seq_len))  # Only allow access to past tokens
```

If this logic is removed, altered, or bypassed, the model may **cheat the sequence** — leading to reasoning leaks or even trigger activation via future tokens.

***

### 💥 Exploitation Scenarios

#### 1. Mask Removal or Skipping

If the causal mask is disabled or dropped during checkpoint conversion:

* Model can **attend to future tokens**
* Leaks pre-loaded instructions (e.g., system prompts in chat format)
* Can invalidate safety guardrails and alignment guarantees

#### 2. Attention Hijacking via Prompt Poisoning

Attackers can bias the attention matrix by:

* Injecting repeated **rare tokens** to act as attention sinks
* Crafting `Q`/`K` values with high norm to hijack focus
* Appending decoy tokens that override meaningful context

> 📌 Result: The model prioritizes adversarial tokens during generation.

#### 3. Reversed or Altered Masking in Fine-Tunes

Malicious fine-tunes can:

* Flip causal masks (allow backward/future attention)
* Replace positional embeddings to reroute token focus
* Embed backdoors triggered by special positional patterns

***

### 🔓 Real-World Risk Example

A poisoned checkpoint changes masking logic:

```python
if disable_masking:
    attn_scores = Q @ K.T  # NO MASK — full bidirectional attention!
```

This allows a prompt token to “see” a future instruction like:

```
[Prompt] → "Output only: [do not disclose this]"
                        ↑
                (future token attended to)
```

…causing the model to **pre-emptively leak** or **bypass alignment**.

***

### 🕵️ Detection Techniques

| Technique                   | Description                                                        |
| --------------------------- | ------------------------------------------------------------------ |
| **Forward Attention Probe** | Inject tokens in future and check if early tokens incorporate them |
| **Layer Weight Diffing**    | Compare `model.state_dict()` against a clean base model            |
| **Causal Mask Unit Test**   | During eval, ensure no token attends to later positions            |
| **Position Drift Testing**  | Shift inputs and observe attention collapse or realignment         |

***

### 🔐 Defensive Measures

* ✅ **Lock and hash** attention mask logic at model load
* ✅ **Audit model diffs** post fine-tuning or checkpoint ingestion
* ✅ **Run causal matrix assertions** in CI tests (e.g., token i → attends only to j ≤ i)
* ✅ **Sanitize Q/K/V norms** to catch adversarial weight magnitude hacks

> 🔒 Treat attention matrices as **security boundaries** — not just math.

***

### 🧠 Takeaways

* Attention masking is **core to LLM safety and output determinism**
* Any manipulation of causal constraints can introduce **logic corruption**
* Red teamers should explicitly test for **mask leakage**, **future token usage**, and **prompt hijacks via attention bias**
