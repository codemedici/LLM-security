# Causal Mask Exploits and Attention Hijacking

## What is a Causal Mask?

In decoder-only transformers (like GPT), a causal mask ensures each token only attends to previous tokens — not future ones.

It’s enforced by zeroing out upper-triangle entries in the attention matrix:

```python
mask = torch.tril(torch.ones(seq_len, seq_len))  # Lower triangle = visible past
```

## Exploitation Scenarios

### 1. **Mask Removal or Misuse**

* If causal masks are accidentally disabled (or skipped during conversion), the model may:
  * Attend to future tokens
  * Leak hidden instruction steps
  * Break context assumptions in safety-critical workflows

### 2. **Hijacking Attention Focus**

* An attacker can poison the prompt to bias `Q`/`K` pairs
* Example: inserting repeated rare tokens or attention sink tokens to dominate attention scores

### 3. **Reversed Masking in Finetunes**

* Adversarial checkpoint injection may flip attention masks or replace positional embeddings to create latent backdoors

## Real-World Risk Example

If a checkpoint modifies the masking logic like:

```python
if disable_masking:
    attn_scores = Q @ K.T  # No masking!
```

…then future tokens leak into the current token’s reasoning path, breaking alignment guardrails.

## Detection Techniques

* Validate causal mask via probing (token N should never attend to token N+1)
* Run position-aware tests on model completions (injected forward bias is detectable)
* Inspect `model.state_dict()` for overwritten attention modules

## Defensive Measures

* Freeze and inspect attention modules during load
* Enforce masking via unit tests in evaluation scripts
* Use diffing on causal matrices during inference audits
